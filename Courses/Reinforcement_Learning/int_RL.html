<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!-- saved from url=(0030)https://sarwanpasha.github.io/ -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <link src="images/cthulu.png" rel="shortcut icon" type="image/x-icon">

    <title>Sarwan Ali</title>
    <style type="text/css">
        * {
            margin: 0;
            padding: 0;
        }

        body {
            font: 16px Helvetica, Sans-Serif;
            line-height: 24px;
            background: url(images/noise.jpg);
        }

        .clear {
            clear: both;
        }

        #page-wrap {
            width: 800px;
            margin: 40px auto 60px;
        }

        #pic {
            float: right;
            margin: -30px 0 0 0;
        }

        h1 {
            margin: 0 0 16px 0;
            padding: 0 0 16px 0;
            font-size: 42px;
            font-weight: bold;
            letter-spacing: -2px;
            border-bottom: 1px solid #999;
        }

        h2 {
            font-size: 20px;
            margin: 0 0 6px 0;
            position: relative;
        }

            h2 span {
                position: absolute;
                bottom: 0;
                right: 0;
                font-style: italic;
                font-family: Georgia, Serif;
                font-size: 16px;
                color: #999;
                font-weight: normal;
            }

        p {
            margin: 0 0 16px 0;
        }

        a {
            color: #999;
            text-decoration: none;
            border-bottom: 1px dotted #999;
        }

            a:hover {
                border-bottom-style: solid;
                color: black;
            }

        ul {
            margin: 0 0 32px 17px;
        }

        #objective {
            width: 500px;
            float: left;
        }

            #objective p {
                font-family: Georgia, Serif;
                font-style: italic;
                color: #666;
            }

        dt {
            font-style: italic;
            font-weight: bold;
            font-size: 18px;
            text-align: right;
            padding: 0 26px 0 0;
            width: 150px;
            float: left;
            height: 100px;
            border-right: 1px solid #999;
        }

        dd {
            width: 600px;
            float: right;
        }

            dd.clear {
                float: none;
                margin: 0;
                height: 15px;
            }
    </style>
	
	<style>
	/* Add a black background color to the top navigation */
.topnav {
  background-color: #333;
  overflow: hidden;
}

/* Style the links inside the navigation bar */
.topnav a {
  float: left;
  color: #f2f2f2;
  text-align: center;
  padding: 14px 16px;
  text-decoration: none;
  font-size: 17px;
}

/* Change the color of links on hover */
.topnav a:hover {
  background-color: #ddd;
  color: black;
}

/* Add a color to the active/current link */
.topnav a.active {
  background-color: #4CAF50;
  color: white;
}
	</style>
	
	<style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
        }
        th, td {
            padding: 8px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }
        th {
            background-color: #f2f2f2;
        }
    </style>
	
    <script>
        function countdown() {
            var deadlines = document.getElementsByClassName('countdown');
            for (var i = 0; i < deadlines.length; i++) {
                var deadlineDate = new Date(deadlines[i].dataset.deadline);
                var currentDate = new Date();
                var timeRemaining = deadlineDate - currentDate;

                if (timeRemaining > 0) {
                    var days = Math.floor(timeRemaining / (1000 * 60 * 60 * 24));
                    var hours = Math.floor((timeRemaining % (1000 * 60 * 60 * 24)) / (1000 * 60 * 60));
                    var minutes = Math.floor((timeRemaining % (1000 * 60 * 60)) / (1000 * 60));
                    var seconds = Math.floor((timeRemaining % (1000 * 60)) / 1000);

                    deadlines[i].innerHTML = days + "d " + hours + "h " + minutes + "m " + seconds + "s";
                } else {
                    deadlines[i].innerHTML = "Deadline passed";
                }
            }

            setTimeout(countdown, 1000);
        }

        window.onload = countdown;
    </script>
</head>

<body data-new-gr-c-s-check-loaded="14.990.0" data-gr-ext-installed="">
    <div id="page-wrap">

       

	   <div class="topnav">
  <a href="https://sarwanpasha.github.io/">Home</a>
  <a href="https://sarwanpasha.github.io/Sarwan_Ali_CV.pdf" target="_blank">CV</a>
  <a href="https://scholar.google.com/citations?user=9dtXSoAAAAAJ&hl=en" target="_blank">Google Scholar</a>
  <!-- <a href="https://sarwanpasha.github.io/conferences.html">Conferences</a> -->
  <a href="https://sarwanpasha.github.io/Courses/course_home.html">Teaching</a>
  <a href="https://sarwanpasha.github.io/PapersDiscussion.html">Papers Discussion</a>
  
</div>
 <br>
  <br>

  <!-- <td>Here I discuss interesting papers of mine and from other authors</td> -->
  <h2>Introduction To Reinforcement Learning</h2>
  <!-- <h1>Interesting Conferences</h1> -->
    <table>
        <thead>
            <tr>
                <th>Topic</th>
                <th>Slides</th>
                <th>Video</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><a style="color:violet">Topic 1.1: Foundations</a>
		<br>
		<strong>What is reinforcement learning? Agent-environment interaction</strong>
		</td>
                <td> <a class="email" href="https://sarwanpasha.github.io/Courses/Reinforcement_Learning/FO_1.pdf" target="_blank" style="color:blue">Slides</a>  </td>
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 1.2: Foundations</a>
		<br>
		<strong>Comparison with supervised and unsupervised learning</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 1.3: Foundations</a>
		<br>
		<strong>Key concepts: rewards, states, actions, policies</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 1.4: Foundations</a>
		<br>
		<strong>Examples of RL applications (games, robotics, recommendation systems)</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 2.1: Markov Decision Processes (MDPs)</a>
		<br>
		<strong>Markov property and Markov chains</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 2.2: Markov Decision Processes (MDPs)</a>
		<br>
		<strong>Finite MDPs: states, actions, rewards, transition probabilities</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 2.3: Markov Decision Processes (MDPs)</a>
		<br>
		<strong>Return, discounting, and value functions</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 2.4: Markov Decision Processes (MDPs)</a>
		<br>
		<strong>Bellman equations for state and action values</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 2.5: Markov Decision Processes (MDPs)</a>
		<br>
		<strong>Optimal policies and optimal value functions</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 3.1: Dynamic Programming</a>
		<br>
		<strong>Policy evaluation (prediction problem)</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 3.2: Dynamic Programming</a>
		<br>
		<strong>Policy improvement and policy iteration</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 3.3: Dynamic Programming</a>
		<br>
		<strong>Value iteration</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 3.4: Dynamic Programming</a>
		<br>
		<strong>Asynchronous dynamic programming</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 3.5: Dynamic Programming</a>
		<br>
		<strong>Generalized policy iteration</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 4.1: Model-Free Prediction</a>
		<br>
		<strong>Monte Carlo methods for value estimation</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 4.2: Model-Free Prediction</a>
		<br>
		<strong>Temporal difference (TD) learning</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 4.3: Model-Free Prediction</a>
		<br>
		<strong>TD(0) algorithm</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 4.4: Model-Free Prediction</a>
		<br>
		<strong>Comparison of MC and TD methods</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 4.5: Model-Free Prediction</a>
		<br>
		<strong>n-step TD methods</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 5.1: Model-Free Control</a>
		<br>
		<strong>Monte Carlo control methods</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 5.2: Model-Free Control</a>
		<br>
		<strong>On-policy vs off-policy learning</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 5.3: Model-Free Control</a>
		<br>
		<strong>SARSA (State-Action-Reward-State-Action)</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 5.4: Model-Free Control</a>
		<br>
		<strong>Q-learning</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 5.5: Model-Free Control</a>
		<br>
		<strong>Expected SARSA</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 5.6: Model-Free Control</a>
		<br>
		<strong>Exploration vs exploitation strategies (ε-greedy, softmax)</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 6.1: Function Approximation</a>
		<br>
		<strong>Need for function approximation in large state spaces</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 6.2: Function Approximation</a>
		<br>
		<strong>Linear function approximation</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 6.3: Function Approximation</a>
		<br>
		<strong>Gradient Monte Carlo and TD methods</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 6.4: Function Approximation</a>
		<br>
		<strong>Feature construction and basis functions</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 6.5: Function Approximation</a>
		<br>
		<strong>Convergence issues with function approximation</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 7.1: Deep Reinforcement Learning</a>
		<br>
		<strong>Neural networks as function approximators</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 7.2: Deep Reinforcement Learning</a>
		<br>
		<strong>Deep Q-Networks (DQN)</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 7.3: Deep Reinforcement Learning</a>
		<br>
		<strong>Experience replay and target networks</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 7.4: Deep Reinforcement Learning</a>
		<br>
		<strong>Double DQN, Dueling DQN</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 7.5: Deep Reinforcement Learning</a>
		<br>
		<strong>Policy gradient methods introduction</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 8.1: Policy Gradient Methods</a>
		<br>
		<strong>REINFORCE algorithm</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 8.2: Policy Gradient Methods</a>
		<br>
		<strong>Actor-critic methods</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 8.3: Policy Gradient Methods</a>
		<br>
		<strong>Advantage functions</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 8.4: Policy Gradient Methods</a>
		<br>
		<strong>Proximal Policy Optimization (PPO) overview</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 8.5: Policy Gradient Methods</a>
		<br>
		<strong>Trust Region Policy Optimization (TRPO) concepts</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 9.1: Advanced Topics</a>
		<br>
		<strong>Multi-armed bandits</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 9.2: Advanced Topics</a>
		<br>
		<strong>Exploration strategies (UCB, Thompson sampling)</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 9.3: Advanced Topics</a>
		<br>
		<strong>Partially observable environments (POMDP introduction)</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 9.4: Advanced Topics</a>
		<br>
		<strong>Hierarchical reinforcement learning basics</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 10.1: Applications and Case Studies</a>
		<br>
		<strong>Game playing (AlphaGo, chess, Atari games)</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 10.2: Applications and Case Studies</a>
		<br>
		<strong>Robotics applications</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 10.3: Applications and Case Studies</a>
		<br>
		<strong>Autonomous vehicles</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 10.4: Applications and Case Studies</a>
		<br>
		<strong>Resource allocation and scheduling</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:green">Topic 10.5: Applications and Case Studies</a>
		<br>
		<strong>Financial trading</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 11.1: Current Research and Future Directions</a>
		<br>
		<strong>Meta-learning in RL</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 11.2: Current Research and Future Directions</a>
		<br>
		<strong>Multi-agent reinforcement learning</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 11.3: Current Research and Future Directions</a>
		<br>
		<strong>Safe reinforcement learning</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 11.4: Current Research and Future Directions</a>
		<br>
		<strong>Real-world deployment challenges</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>

	    <tr>
                <td><a style="color:violet">Topic 11.5: Current Research and Future Directions</a>
		<br>
		<strong>Open research problems</strong> 
		</td>
		<td> <a> - </a> </td> 
                <td> <a> - </a> </td>
            </tr>
		

        </tbody>
    </table>
	
	
</body></html>
